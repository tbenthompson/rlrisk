Plan:
* (CHECK) simple Risk engine
* (CHECK) encode the game state as a vector
* (CHECK) try REINFORCE
* (CHECK) add the one hot encoded player idx to the state vector
* (CHECK) rust-level capability to run many games at once.
* (CHECK) profile the training, any easy gains in performance? 
* (CHECK) robust self-play
* (CHECK) implement actor-critic VPG
* (CHECK) pytorch on gpu
* (CHECK) fix self-play implementation to train on all samples at once
* log KL divergence
* implement lambda-GAE
* try making the learning player always player 0 from the state vector.
* use a variant of the game that encourages short games.
* implement PPO
* implement KL divergence check/early-stopping
* more robust way of determining speed of learning/quality of algorithm
* try methods against openai gym to check performance.
* parallelize play_games
* play games against earlier models, league play
* try adding connectivity
* try adding continents
* try adding reinforcement phase
* "transfer learning" from gamespec 1 to gamespec 2
* ugly gui to play human against computer
* refactor/improve the risk engine.
* cool idea: what if we just convert the Board struct to binary?!

Dimensions on which we can simplify Risk rules:
* fewer territories (simple == 2 or 3)
* territory connectivity (simple == fully connected)
* fewer players (simple == 2)
* number of reinforcements (simple == constant)
* continents (simple == no continents)
* number of cards (simple == 0)
* number of attacks allowed per turn (simple == 1)
* move full stack vs choice in move size (simple == move full stack)
* fortification (simple == no fortification)

Common Risk variations
* fixed vs progressive cards
* blizzards
* fog of war
* auto vs manual starting placement
* different maps

