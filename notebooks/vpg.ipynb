{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "willing-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_spec = dict(\n",
    "    n_players=3,\n",
    "    n_territories=8,\n",
    "    baseline_reinforcements=3,\n",
    "    n_attacks_per_turn=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "least-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import risk_ext\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def vec_to_matrix(game, v):\n",
    "    return v.reshape((game.n_max_territories, 1 + game.n_max_players))\n",
    "\n",
    "def start_game(seed):\n",
    "    return risk_ext.start_game(game_spec['n_players'], game_spec['n_territories'], game_spec['baseline_reinforcements'], game_spec['n_attacks_per_turn'], 0)\n",
    "\n",
    "def get_state_dim():\n",
    "    tmp = start_game(0)\n",
    "    return tmp.n_max_territories * (1 + tmp.n_max_players)\n",
    "\n",
    "def play_game(players, verbose=False, max_turns=None):\n",
    "    env.reset()\n",
    "    while True:\n",
    "        action = players[env.game.player_idx].act(env.game, env.game.board_state)\n",
    "        if verbose:\n",
    "            print(f'turn={env.game.turn_idx}, player={env.game.player_idx}, phase={env.game.phase}, action={action}, board={env.game.board_state}')\n",
    "        _,_,done = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        if max_turns is not None:\n",
    "            if env.game.turn_idx > max_turns:\n",
    "                break\n",
    "    return env.game.player_idx\n",
    "\n",
    "def faceoff(players, n_games):\n",
    "    winners = np.empty(n_games, dtype = np.int32)\n",
    "    for i in range(n_games):\n",
    "        winners[i] = play_game(players)\n",
    "    winner_counts = np.unique(winners, return_counts=True)\n",
    "    return winners, winner_counts[0], winner_counts[1] / n_games\n",
    "\n",
    "class NNPlayer:\n",
    "    def __init__(self):\n",
    "        self.state_dim = get_state_dim()\n",
    "        self.n_actions = game_spec['n_territories']\n",
    "        self.lr = 0.01\n",
    "        \n",
    "        def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "            # Build a feedforward neural network.\n",
    "            layers = []\n",
    "            for j in range(len(sizes)-1):\n",
    "                act = activation if j < len(sizes)-2 else output_activation\n",
    "                layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "            return nn.Sequential(*layers)\n",
    "        \n",
    "        self.logits_net = mlp(sizes=[self.state_dim]+[32]+[self.n_actions])\n",
    "        self.optimizer = Adam(self.logits_net.parameters(), lr=self.lr)\n",
    "\n",
    "    def get_policy(self, obs):\n",
    "        logits = self.logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        return self.get_policy(obs).sample().item()\n",
    "\n",
    "    def compute_loss(self, obs, act, weights):\n",
    "        logp = self.get_policy(obs).log_prob(act)\n",
    "        return -(logp * weights).mean()\n",
    "    \n",
    "    def act(self, game, state_vec):\n",
    "        owner_col = vec_to_matrix(game, state_vec)[:, game.player_idx + 1]\n",
    "        attack_from = (owner_col == 1).argmax()\n",
    "        attack_to = self.get_action(torch.as_tensor(state_vec, dtype=torch.float32))\n",
    "        return attack_from, attack_to\n",
    "    \n",
    "    def learn(self, obs, actions, weights):\n",
    "        self.optimizer.zero_grad()\n",
    "        batch_loss = self.compute_loss(obs=torch.as_tensor(obs, dtype=torch.float32),\n",
    "                                  act=torch.as_tensor(actions, dtype=torch.int32),\n",
    "                                  weights=torch.as_tensor(weights, dtype=torch.float32)\n",
    "                                  )\n",
    "        batch_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return batch_loss\n",
    "\n",
    "class DumbPlayer:\n",
    "    def act(self, game, state_vec):\n",
    "        owner_col = vec_to_matrix(game, state_vec)[:, game.player_idx + 1]\n",
    "        attack_from = (owner_col == 1).argmax()\n",
    "        attack_to = (owner_col != 1).argmax()\n",
    "        return attack_from, attack_to\n",
    "\n",
    "    def learn(self, obs, actions, weights):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "international-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.n_games = 0\n",
    "        \n",
    "    def reset(self, seed=None):\n",
    "        self.n_games += 1\n",
    "        if seed is None:\n",
    "            seed = self.n_games\n",
    "        self.game = start_game(seed)\n",
    "        return self.game.board_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.game.step(*action)\n",
    "        done = self.game.phase == 3\n",
    "        return self.game.board_state, float(done), done\n",
    "\n",
    "class PlayerBatch:\n",
    "    def __init__(self):\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.weights = []\n",
    "        self.returns = []\n",
    "        self.lengths = []\n",
    "        self.start_episode()\n",
    "    \n",
    "    def record(self, obs, action):      \n",
    "        self.obs.append(obs.copy())\n",
    "        self.actions.append(action[1])\n",
    "        self.ep_length += 1\n",
    "    \n",
    "    def finish_episode(self, reward):\n",
    "        self.ep_returns = reward\n",
    "        self.returns.append(self.ep_returns)\n",
    "        self.lengths.append(self.ep_length)\n",
    "        self.weights += [self.ep_returns] * self.ep_length\n",
    "        self.start_episode()\n",
    "    \n",
    "    def start_episode(self):\n",
    "        self.ep_length = 0\n",
    "        self.ep_returns = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "representative-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(env, players, batch_size):\n",
    "    batches = [PlayerBatch() for p in players]\n",
    "    obs = env.reset()\n",
    "\n",
    "    go = True\n",
    "    while go:\n",
    "        #print(env.game.turn_idx, env.game.player_idx)\n",
    "        action = players[env.game.player_idx].act(env.game, obs)\n",
    "        batches[env.game.player_idx].record(obs, action)\n",
    "        obs, reward, done = env.step(action)\n",
    "        if done:\n",
    "            for i in range(len(players)):\n",
    "                player_reward = reward if i == env.game.player_idx else 0\n",
    "                b = batches[i]\n",
    "                b.finish_episode(player_reward)\n",
    "                if len(b.obs) > batch_size:\n",
    "                    go = False\n",
    "            obs = env.reset()\n",
    "            \n",
    "    loss = []\n",
    "    for i in range(len(players)):\n",
    "        if len(batches[i].obs) == 0:\n",
    "            loss.append(0)\n",
    "        else:\n",
    "            loss.append(players[i].learn(batches[i].obs, batches[i].actions, batches[i].weights))\n",
    "    return loss, [b.returns for b in batches], [b.lengths for b in batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dynamic-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, players, n_batches, batch_size, print_players):\n",
    "    for i in range(n_batches):\n",
    "        loss, rets, lens = train_one_epoch(env, players, batch_size)\n",
    "        for j in print_players:\n",
    "            win_percentage = np.mean(rets[j])\n",
    "            game_length = np.mean(lens[j])\n",
    "            print(f'epoch: {i}, player: {j}, loss: {loss[j]:.3f} return: {win_percentage:.3f} ep_len: {game_length:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amended-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env()\n",
    "players = [NNPlayer()] + [DumbPlayer() for i in range(game_spec['n_players'])]\n",
    "nn_player_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ethical-baghdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, player: 0, loss: 0.744 return: 0.500 ep_len: 330.700\n",
      "epoch: 1, player: 0, loss: 1.299 return: 0.900 ep_len: 363.800\n",
      "epoch: 2, player: 0, loss: 1.109 return: 0.833 ep_len: 508.000\n",
      "epoch: 3, player: 0, loss: 1.141 return: 0.889 ep_len: 392.778\n",
      "epoch: 4, player: 0, loss: 1.237 return: 0.750 ep_len: 446.500\n",
      "epoch: 5, player: 0, loss: 1.205 return: 0.800 ep_len: 649.400\n",
      "epoch: 6, player: 0, loss: 0.934 return: 0.800 ep_len: 671.600\n",
      "epoch: 7, player: 0, loss: 1.190 return: 0.833 ep_len: 521.667\n",
      "epoch: 8, player: 0, loss: 1.206 return: 0.714 ep_len: 487.143\n",
      "epoch: 9, player: 0, loss: 1.273 return: 1.000 ep_len: 614.800\n",
      "epoch: 10, player: 0, loss: 1.305 return: 1.000 ep_len: 750.250\n",
      "epoch: 11, player: 0, loss: 1.279 return: 1.000 ep_len: 747.000\n",
      "epoch: 12, player: 0, loss: 1.092 return: 0.667 ep_len: 526.000\n",
      "epoch: 13, player: 0, loss: 1.288 return: 1.000 ep_len: 1881.000\n",
      "epoch: 14, player: 0, loss: 1.233 return: 1.000 ep_len: 1192.000\n",
      "epoch: 15, player: 0, loss: 1.257 return: 1.000 ep_len: 1430.000\n",
      "epoch: 16, player: 0, loss: 1.296 return: 1.000 ep_len: 1212.333\n",
      "epoch: 17, player: 0, loss: 1.270 return: 1.000 ep_len: 841.000\n",
      "epoch: 18, player: 0, loss: 1.262 return: 1.000 ep_len: 852.000\n",
      "epoch: 19, player: 0, loss: 1.244 return: 0.800 ep_len: 1017.600\n",
      "epoch: 20, player: 0, loss: 1.178 return: 0.750 ep_len: 1181.250\n",
      "epoch: 21, player: 0, loss: 1.103 return: 0.667 ep_len: 1004.667\n",
      "epoch: 22, player: 0, loss: 0.864 return: 0.571 ep_len: 548.714\n",
      "epoch: 23, player: 0, loss: 1.279 return: 1.000 ep_len: 1200.000\n",
      "epoch: 24, player: 0, loss: 1.171 return: 1.000 ep_len: 3730.000\n",
      "epoch: 25, player: 0, loss: 0.960 return: 0.667 ep_len: 1005.000\n",
      "epoch: 26, player: 0, loss: 1.093 return: 1.000 ep_len: 1048.667\n",
      "epoch: 27, player: 0, loss: 0.821 return: 0.375 ep_len: 438.500\n",
      "epoch: 28, player: 0, loss: 0.721 return: 0.667 ep_len: 1085.667\n",
      "epoch: 29, player: 0, loss: 1.090 return: 1.000 ep_len: 3258.000\n",
      "epoch: 30, player: 0, loss: 0.698 return: 0.500 ep_len: 1515.000\n",
      "epoch: 31, player: 0, loss: 0.547 return: 0.167 ep_len: 909.167\n",
      "epoch: 32, player: 0, loss: 0.619 return: 0.250 ep_len: 376.000\n",
      "epoch: 33, player: 0, loss: 0.656 return: 0.750 ep_len: 980.000\n",
      "epoch: 34, player: 0, loss: 0.566 return: 1.000 ep_len: 2993.333\n",
      "epoch: 35, player: 0, loss: 0.509 return: 0.667 ep_len: 1472.667\n",
      "epoch: 36, player: 0, loss: 0.673 return: 0.500 ep_len: 558.750\n",
      "epoch: 37, player: 0, loss: 0.665 return: 0.667 ep_len: 1026.667\n",
      "epoch: 38, player: 0, loss: 0.620 return: 0.600 ep_len: 1015.200\n",
      "epoch: 39, player: 0, loss: 0.486 return: 0.750 ep_len: 2637.750\n",
      "epoch: 40, player: 0, loss: 0.480 return: 0.500 ep_len: 633.000\n",
      "epoch: 41, player: 0, loss: 0.409 return: 1.000 ep_len: 9677.000\n",
      "epoch: 42, player: 0, loss: 0.510 return: 0.600 ep_len: 724.400\n",
      "epoch: 43, player: 0, loss: 0.341 return: 0.500 ep_len: 2580.000\n",
      "epoch: 44, player: 0, loss: 0.424 return: 0.750 ep_len: 1408.000\n",
      "epoch: 45, player: 0, loss: 0.431 return: 1.000 ep_len: 3622.000\n",
      "epoch: 46, player: 0, loss: 0.433 return: 1.000 ep_len: 1687.500\n",
      "epoch: 47, player: 0, loss: 0.329 return: 1.000 ep_len: 3335.500\n",
      "epoch: 48, player: 0, loss: 0.270 return: 1.000 ep_len: 4359.000\n",
      "epoch: 49, player: 0, loss: 0.437 return: 1.000 ep_len: 1676.250\n"
     ]
    }
   ],
   "source": [
    "train(env, players, 50, 3000, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-description",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, player: 0, loss: 0.574 return: 0.667 ep_len: 1723.667\n",
      "epoch: 0, player: 1, loss: -0.000 return: 0.000 ep_len: 1690.000\n",
      "epoch: 0, player: 2, loss: 0.444 return: 0.333 ep_len: 808.667\n",
      "epoch: 1, player: 0, loss: -0.000 return: 0.000 ep_len: 2005.000\n",
      "epoch: 1, player: 1, loss: 0.441 return: 0.500 ep_len: 2994.500\n",
      "epoch: 1, player: 2, loss: 0.648 return: 0.500 ep_len: 1676.500\n"
     ]
    }
   ],
   "source": [
    "selfplayer = copy.deepcopy(players[0])\n",
    "selfplayers = [selfplayer] * 3\n",
    "train(env, selfplayers, 10, 5000, [0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env, [DumbPlayer(), NNPlayer(), DumbPlayer()], 50, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env, [DumbPlayer(), copy.deepcopy(players[0]), DumbPlayer()], 50, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_player_idx = 0\n",
    "for i in range(50):\n",
    "    loss, rets, lens = train_one_epoch(env, players, 4000)\n",
    "    for pi in range(len(players)):\n",
    "        print('epoch: {%3d} \\t loss0: %.3f \\t return0: %.3f \\t ep_len: %.3f'%\n",
    "                (i, loss[pi], np.mean(rets[pi]), np.mean(lens[pi])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "faceoff(selfplayers, 100)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lined-blind",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int32), array([0.95, 0.05]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faceoff(players, 100)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "faceoff([DumbPlayer() for i in range(game_spec['n_players'])], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "variable-nutrition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turn=0, player=0, phase=1, action=(1, 1), board=[8. 0. 0. 1. 8. 1. 0. 0. 3. 1. 0. 0. 8. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=0, player=0, phase=1, action=(1, 1), board=[8. 0. 0. 1. 8. 1. 0. 0. 3. 1. 0. 0. 8. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=0, player=0, phase=1, action=(1, 1), board=[8. 0. 0. 1. 8. 1. 0. 0. 3. 1. 0. 0. 8. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=0, player=0, phase=1, action=(1, 2), board=[8. 0. 0. 1. 8. 1. 0. 0. 3. 1. 0. 0. 8. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=0, player=0, phase=1, action=(1, 1), board=[8. 0. 0. 1. 8. 1. 0. 0. 3. 1. 0. 0. 8. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=0, player=1, phase=1, action=(3, 0), board=[ 8.  0.  0.  1.  8.  1.  0.  0.  3.  1.  0.  0. 11.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "turn=0, player=1, phase=1, action=(3, 0), board=[ 6.  0.  0.  1.  8.  1.  0.  0.  3.  1.  0.  0. 11.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "turn=0, player=1, phase=1, action=(3, 0), board=[6. 0. 0. 1. 8. 1. 0. 0. 3. 1. 0. 0. 9. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=0, player=1, phase=1, action=(3, 0), board=[6. 0. 0. 1. 8. 1. 0. 0. 3. 1. 0. 0. 7. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=0, player=1, phase=1, action=(3, 0), board=[6. 0. 0. 1. 8. 1. 0. 0. 3. 1. 0. 0. 5. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=0, player=2, phase=1, action=(0, 1), board=[9. 0. 0. 1. 8. 1. 0. 0. 3. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=0, player=2, phase=1, action=(0, 1), board=[9. 0. 0. 1. 6. 1. 0. 0. 3. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=0, player=2, phase=1, action=(0, 1), board=[9. 0. 0. 1. 4. 1. 0. 0. 3. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=0, player=2, phase=1, action=(0, 1), board=[8. 0. 0. 1. 3. 1. 0. 0. 3. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=0, player=2, phase=1, action=(0, 1), board=[6. 0. 0. 1. 3. 1. 0. 0. 3. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=1, player=0, phase=1, action=(1, 0), board=[6. 0. 0. 1. 4. 1. 0. 0. 3. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=1, player=0, phase=1, action=(1, 2), board=[5. 0. 0. 1. 3. 1. 0. 0. 3. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=1, player=0, phase=1, action=(1, 2), board=[5. 0. 0. 1. 3. 1. 0. 0. 3. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=1, player=0, phase=1, action=(1, 0), board=[5. 0. 0. 1. 3. 1. 0. 0. 3. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=1, player=0, phase=1, action=(1, 0), board=[5. 0. 0. 1. 1. 1. 0. 0. 3. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=1, player=1, phase=1, action=(3, 0), board=[5. 0. 0. 1. 1. 1. 0. 0. 3. 1. 0. 0. 6. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=1, player=1, phase=1, action=(3, 0), board=[4. 0. 0. 1. 1. 1. 0. 0. 3. 1. 0. 0. 5. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=1, player=1, phase=1, action=(3, 0), board=[3. 0. 0. 1. 1. 1. 0. 0. 3. 1. 0. 0. 4. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=1, player=1, phase=1, action=(3, 0), board=[3. 0. 0. 1. 1. 1. 0. 0. 3. 1. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=1, player=1, phase=1, action=(3, 0), board=[3. 0. 0. 1. 1. 1. 0. 0. 3. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=1, player=2, phase=1, action=(0, 1), board=[6. 0. 0. 1. 1. 1. 0. 0. 3. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=1, player=2, phase=1, action=(0, 1), board=[5. 0. 0. 1. 1. 1. 0. 0. 3. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=1, player=2, phase=1, action=(0, 2), board=[1. 0. 0. 1. 4. 0. 0. 1. 3. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=1, player=2, phase=1, action=(0, 2), board=[1. 0. 0. 1. 4. 0. 0. 1. 3. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=1, player=2, phase=1, action=(0, 2), board=[1. 0. 0. 1. 4. 0. 0. 1. 3. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=2, player=0, phase=1, action=(2, 0), board=[1. 0. 0. 1. 4. 0. 0. 1. 6. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=2, player=0, phase=1, action=(0, 0), board=[5. 1. 0. 0. 4. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=2, player=0, phase=1, action=(0, 1), board=[5. 1. 0. 0. 4. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=2, player=0, phase=1, action=(0, 1), board=[5. 1. 0. 0. 2. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=2, player=0, phase=1, action=(0, 0), board=[3. 1. 0. 0. 2. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=2, player=1, phase=1, action=(3, 0), board=[3. 1. 0. 0. 2. 0. 0. 1. 1. 1. 0. 0. 4. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=2, player=1, phase=1, action=(3, 0), board=[3. 1. 0. 0. 2. 0. 0. 1. 1. 1. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=2, player=1, phase=1, action=(3, 0), board=[2. 1. 0. 0. 2. 0. 0. 1. 1. 1. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=2, player=1, phase=1, action=(3, 0), board=[2. 1. 0. 0. 2. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=2, player=1, phase=1, action=(3, 0), board=[2. 1. 0. 0. 2. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=2, player=2, phase=1, action=(1, 0), board=[2. 1. 0. 0. 5. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=2, player=2, phase=1, action=(1, 0), board=[1. 1. 0. 0. 4. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=2, player=2, phase=1, action=(0, 2), board=[3. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=2, player=2, phase=1, action=(0, 3), board=[1. 0. 0. 1. 1. 0. 0. 1. 2. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=2, player=2, phase=1, action=(0, 3), board=[1. 0. 0. 1. 1. 0. 0. 1. 2. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=3, player=1, phase=1, action=(3, 0), board=[1. 0. 0. 1. 1. 0. 0. 1. 2. 0. 0. 1. 4. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=3, player=1, phase=1, action=(3, 0), board=[1. 0. 0. 1. 1. 0. 0. 1. 2. 0. 0. 1. 3. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=3, player=1, phase=1, action=(0, 1), board=[2. 0. 1. 0. 1. 0. 0. 1. 2. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=3, player=1, phase=1, action=(0, 1), board=[1. 0. 1. 0. 1. 0. 0. 1. 2. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=3, player=1, phase=1, action=(0, 1), board=[1. 0. 1. 0. 1. 0. 0. 1. 2. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=3, player=2, phase=1, action=(1, 0), board=[1. 0. 1. 0. 4. 0. 0. 1. 2. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "turn=3, player=2, phase=1, action=(0, 3), board=[3. 0. 0. 1. 1. 0. 0. 1. 2. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-01dc11fa3e2c>:74: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1614378073166/work/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  attack_to = self.get_action(torch.as_tensor(state_vec, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_game(players, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-victoria",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
